{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Advanced Statistical Machine Learning & Pattern Recognition - CO495</h1>\n",
    "<h2 align=\"center\">Coursework 2 - Hidden Markov Models </h2>\n",
    "<h3 align=\"center\">Ilaria Manco </h3> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions and classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize(A, dim=None, precision=1e-9):\n",
    "    \"\"\"This function is adapted from Kevin Murphy's code for Machine Learning: a Probabilistic Perspective.\n",
    "\n",
    "    Make the entries of a (multidimensional) array sum to 1\n",
    "    A, z = normalize(A) normalize the whole array, where z is the normalizing constant\n",
    "    A, z = normalize(A, dim)\n",
    "    If dim is specified, we normalize the specified dimension only.\n",
    "    dim=0 means each column sums to one\n",
    "    dim=1 means each row sums to one\n",
    "\n",
    "\n",
    "    Set any zeros to one before dividing.\n",
    "    This is valid, since s=0 iff all A(i)=0, so\n",
    "    we will get 0/1=0\n",
    "\n",
    "    Adapted from https://github.com/probml/pmtk3\"\"\"\n",
    "    \n",
    "    if dim is not None and dim > 1:\n",
    "        raise ValueError(\"Normalize doesn't support more than two dimensions.\")\n",
    "    \n",
    "    z = A.sum(dim)\n",
    "    # If z is a scalar, z.shape is an empty tuple and evaluates to False\n",
    "    if z.shape:\n",
    "        z[np.abs(z) < precision] = 1\n",
    "    elif np.abs(z) < precision:\n",
    "        return 0, 1\n",
    "    \n",
    "    if dim == 1:\n",
    "        return np.transpose(A.T / z), z\n",
    "    else:\n",
    "        return A / z, z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initial values are provided as namedtuples (initialization.A is the initial value for A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "InitGaussian = namedtuple('InitGaussian', ['A', 'Means', 'Variances', 'pi'])\n",
    "InitMultinomial = namedtuple('InitMultinomial', ['A', 'B', 'pi'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering and Smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The core of EM estimation on HMM operates on vectors of probabilities, so the main difference between EM for Gaussian HMM and multinomial HMM is the computation of the observation probabilities and which parameters to estimate.\n",
    "\n",
    "The two functions below compute the probabilities of the data for a given observation model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def computeSmallB_Gaussian(Y, Means, Variances, Nhidden, T):\n",
    "    \"\"\"Compute the probabilities for the data points Y for a Gaussian observation model \n",
    "        with parameters Means and Variances.\n",
    "        \n",
    "        Input parameters:\n",
    "            - Y: the data\n",
    "            - Means: vector of the current estimates of the means\n",
    "            - Variances: vector of the current estimates of the variances\n",
    "            - Nhidden: number of hidden states\n",
    "            - T: length of the sequence\n",
    "        Output:\n",
    "            - b: vector of observation probabilities\n",
    "    \"\"\"\n",
    "    \n",
    "    b = np.zeros((Nhidden, T))\n",
    "    \n",
    "    for i in range(Nhidden):\n",
    "        pdf = norm.pdf(Y, Means[i], np.sqrt(Variances[i]))\n",
    "        b[i] = pdf\n",
    "                   \n",
    "    return normalize(b, dim = 0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def computeSmallB_Discrete(Y, B):\n",
    "    \"\"\"Compute the probabilities for the data points Y for a multinomial observation model \n",
    "        with observation matrix B\n",
    "        \n",
    "        Input parameters:\n",
    "            - Y: the data\n",
    "            - B: matrix of observation probabilities\n",
    "        Output:\n",
    "            - b: vector of observation probabilities\n",
    "    \"\"\"\n",
    "    k = len(B)\n",
    "    N = len(Y)\n",
    "    \n",
    "    b = np.zeros((k, N))\n",
    "\n",
    "    for i in range(k):\n",
    "        for n in range(N):\n",
    "            b[i][n] = B[i][int(Y[n]-1)]\n",
    "    \n",
    "    return normalize(b, dim = 0)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smoothing and filtering: Estimation step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The E step involves smoothing and filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def BackwardFiltering(A, b, N, T):\n",
    "    \"\"\"Perform backward filtering.\n",
    "        Input parameters:\n",
    "            - A: estimated transition matrix (between states)\n",
    "            - b: estimated observation probabilities (local evidence vector)\n",
    "            - N: number of hidden states\n",
    "            - T: length of the sequence\n",
    "        Output:\n",
    "            - beta: filtered probabilities\n",
    "    \"\"\"\n",
    "    beta = np.ones((N, T))\n",
    "    \n",
    "    for t in range(T-1, -1, -1):\n",
    "        beta[:, t-1] = np.dot(A,beta[:,t]*b[:,t])\n",
    "        \n",
    "    #normalise\n",
    "    beta = beta/np.sum(beta, axis = 0)\n",
    "        \n",
    "    return beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ForwardFiltering(A, b, pi, N, T):\n",
    "    \"\"\"Filtering using the forward algorithm (Section 17.4.2 of K. Murphy's book)\n",
    "    Input:\n",
    "      - A: estimated transition matrix\n",
    "      - b: estimated observation probabilities (local evidence vector)\n",
    "      - pi: initial state distribution pi(j) = p(z_1 = j)\n",
    "    Output:\n",
    "      - Filtered belief state at time t: alpha = p(z_t|x_1:t)\n",
    "      - log p(x_1:T)\n",
    "      - Z: normalization constant\"\"\"\n",
    "    \n",
    "    alpha = np.ones((N, T))\n",
    "    Z = np.ones(T)\n",
    "    \n",
    "    alpha[:,0], Z[0] = normalize(np.multiply(b[:,0], pi.T))\n",
    "    \n",
    "    for t in range(1, T):\n",
    "        alpha[:,t], Z[t] = normalize(np.multiply(b[:,t], np.dot(A.T,alpha[:,t-1])))\n",
    "    \n",
    "    logProb = np.sum(np.log(Z))\n",
    "    \n",
    "    return alpha, logProb, Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ForwardBackwardSmoothing(A, b, pi, N, T):\n",
    "    \"\"\"Smoothing using the forward-backward algorithm.\n",
    "    Input:\n",
    "      - A: estimated transition matrix\n",
    "      - b: local evidence vector (observation probabilities)\n",
    "      - pi: initial distribution of states\n",
    "      - N: number of hidden states\n",
    "      - T: length of the sequence\n",
    "    Output:\n",
    "      - alpha: filtered belief state as defined in ForwardFiltering\n",
    "      - beta: conditional likelihood of future evidence as defined in BackwardFiltering\n",
    "      - gamma: gamma_t(j) proportional to alpha_t(j) * beta_t(j)\n",
    "      - lp: log probability defined in ForwardFiltering\n",
    "      - Z: constant defined in ForwardFiltering\"\"\"\n",
    "    \n",
    "    alpha, logProb, Z = ForwardFiltering(A, b, pi, N, T) \n",
    "    beta = BackwardFiltering(A, b, N, T)\n",
    "    \n",
    "    gamma = normalize(alpha*beta, dim =0)[0]\n",
    "    \n",
    "    return alpha, beta, gamma, logProb, Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def SmoothedMarginals(A, b, alpha, beta, T, Nhidden):\n",
    "    \"Two-sliced smoothed marginals p(z_t = i, z_t+1 = j | x_1:T)\"\n",
    "    \n",
    "    marginal = np.zeros((Nhidden, Nhidden, T-1));\n",
    "    \n",
    "    for t in range(T-1):\n",
    "        \n",
    "        product = b[:, t+1] * beta[:, t+1]\n",
    "        product = np.reshape(product, (1,2))\n",
    "\n",
    "        marginal[:, :, t] = normalize(A * np.outer(alpha[:, t],  product.T), dim=1)[0]\n",
    "\n",
    "    return marginal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EM estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the implementation of the EM algorithm.\n",
    "\n",
    "#### Initialisation\n",
    "Here, only the initialisation values provided are used. \n",
    "\n",
    "However, other ways to initialise the parameters include:\n",
    "* randomly choose a set of parameters and run the algorithm multiple times to identify the best set\n",
    "* ignore the Markov dependencies and use other mixture model estimation techniques such as K-Means or EM\n",
    "* use Viterbi training after reaching near convergence with the forwards-backwards training\n",
    "\n",
    "#### Convergence\n",
    "A convergence criterion is applied as follows: at each step, the difference between the logProb obtained from the previous iteration and the current one is computed to check whether this is smaller than $\\epsilon = 10^6$. If this is the case, convergence is reached and the udpated parameters are returned. \n",
    "\n",
    "If not, the iterations continue until they reach the upper bound fixed by the *Niter* argument.\n",
    "\n",
    "\n",
    "#### Performance\n",
    "The performance of the algorithm is evaluated using Viterbi decoding (implemented below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian observation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def EM_estimate_gaussian(Y, Nhidden, Niter, epsilon, init):\n",
    "    \n",
    "    # Dimensions of the data\n",
    "    N, T = Y.shape\n",
    "    \n",
    "    # Initialization\n",
    "    A = init.A\n",
    "    Means = init.Means\n",
    "    Variances = init.Variances;\n",
    "    pi = init.pi\n",
    "    \n",
    "    ###############################################\n",
    "    # EM algorithm\n",
    "    \n",
    "    i = 0\n",
    "        \n",
    "    logProbSum = [0, 1]\n",
    "\n",
    "    while i<Niter and (abs(logProbSum[i+1] - logProbSum[i]) > epsilon): \n",
    "        means_num, den, var_num = np.zeros((Nhidden,1)), np.zeros((Nhidden,1)), np.zeros((Nhidden,1))\n",
    "        pi_new, A_new = np.zeros(np.shape(init.pi)), np.zeros(np.shape(init.A))\n",
    "        \n",
    "        logProbList = []\n",
    "        \n",
    "        for n in range(N):\n",
    "            #select sequence\n",
    "            Y_s = Y[n]\n",
    "            \n",
    "            b = computeSmallB_Gaussian(Y_s, Means, Variances, Nhidden, T)\n",
    "            alpha, beta, gamma, logProb, Z =  ForwardBackwardSmoothing(A, b, pi, Nhidden, T)\n",
    "            marginal = SmoothedMarginals(A, b, alpha, beta, T, Nhidden)\n",
    "                        \n",
    "            means_num +=  np.reshape(np.sum(np.multiply(gamma, Y_s), axis = 1), (Nhidden,1))\n",
    "            den += np.reshape(np.sum(gamma, axis = 1), (Nhidden,1))\n",
    "            var_num += np.reshape(np.sum(np.multiply(gamma, (Y_s - Means)*(Y_s - Means)), axis = 1), (Nhidden,1))\n",
    "            \n",
    "            pi_new += np.reshape(gamma[:,0], (Nhidden, 1))\n",
    "            A_new += np.sum(marginal, axis = 2)\n",
    "            \n",
    "            logProbList.append(logProb)\n",
    "\n",
    "        logProbSum.append(sum(logProbList))\n",
    "        \n",
    "        Means = means_num/den\n",
    "        Variances = var_num/den\n",
    "        A = normalize(A_new, dim =1)[0]\n",
    "        pi = normalize(pi_new)[0] \n",
    "\n",
    "        i += 1    \n",
    "    \n",
    "    print(\"Convergence (Gaussian): \",abs(logProbSum[i+1] - logProbSum[i]) < epsilon)    \n",
    "    return A, Means, Variances, pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial observation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot_encoding(Y):\n",
    "    N = len(np.unique(Y))\n",
    "    T = len(Y)\n",
    "    X = np.zeros((T, N))\n",
    "    \n",
    "    for t in range(T):\n",
    "        X[t][int(Y[t])-1] = 1\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def EM_estimate_multinomial(Y, Nhidden, Niter, epsilon, init):\n",
    "    \n",
    "    # Dimensions of the data\n",
    "    N, T = Y.shape\n",
    "    \n",
    "    # Initialization\n",
    "    A = init.A\n",
    "    B = init.B\n",
    "    pi = init.pi\n",
    "    \n",
    "    ###############################################\n",
    "    # EM algorithm\n",
    "    \n",
    "    i = 0\n",
    "    \n",
    "    logProbSum = [0, 1]\n",
    "    \n",
    "    while (i<Niter) and (abs(logProbSum[i+1] - logProbSum[i]) > epsilon): \n",
    "        A_new, B_new, pi_new = np.zeros(np.shape(init.A)), np.zeros(np.shape(init.B)), np.zeros(np.shape(init.pi))\n",
    "        den = np.zeros((Nhidden, 1))\n",
    "    \n",
    "        logProbList = []\n",
    "\n",
    "        for n in range(N):\n",
    "            #select the sequence\n",
    "            X = one_hot_encoding(Y[n])\n",
    "            \n",
    "            b = computeSmallB_Discrete(Y[n], B)\n",
    "            alpha, beta, gamma, logProb, Z = ForwardBackwardSmoothing(A, b, pi, Nhidden, T)\n",
    "            marginal = SmoothedMarginals(A, b, alpha, beta, T, Nhidden)\n",
    "\n",
    "            B_new += np.dot(gamma, X)\n",
    "            den += np.reshape(np.sum(gamma, axis = 1), (Nhidden,1))\n",
    "            \n",
    "            pi_new += np.reshape(gamma[:,0], (Nhidden, 1))\n",
    "            A_new += np.sum(marginal, axis = 2)\n",
    "\n",
    "            logProbList.append(logProb)\n",
    "\n",
    "        logProbSum.append(sum(logProbList))\n",
    "        B = normalize(B_new/den, dim = 1)[0]       \n",
    "        A = normalize(A_new, dim = 1)[0]\n",
    "        pi = normalize(pi_new)[0]\n",
    "\n",
    "        i += 1\n",
    "    \n",
    "    print(\"Convergence (multinomial): \",abs(logProbSum[i+1] - logProbSum[i]) < epsilon)\n",
    "    return A, B, pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viterbi decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ViterbiDecode(Y, Nhidden, outModel, init):\n",
    "    \n",
    "    if outModel == 'gauss':\n",
    "        A, Mu, Sigma, Pi = EM_estimate_gaussian(Y, Nhidden, 100, 1e-6, init)\n",
    "        smallB = lambda X : computeSmallB_Gaussian(X, Mu, Sigma, Nhidden, len(X))\n",
    "    elif outModel == 'multinomial':\n",
    "        A, B, Pi = EM_estimate_multinomial(Y, Nhidden, 100, 1e-6, init)\n",
    "        smallB = lambda X : computeSmallB_Discrete(X, B)\n",
    "    else:\n",
    "        raise ValueError('Invalid observation model: must be either \"gauss\" or \"multinomial\"')\n",
    "    \n",
    "    # Implement Viterbi decoding here.\n",
    "    N, T = np.shape(Y)\n",
    "    \n",
    "    S = np.ones((N, T))\n",
    "\n",
    "    for n in range(N):\n",
    "        b = smallB(Y[n])\n",
    "        \n",
    "        delta = np.ones((Nhidden, T))\n",
    "        psi = np.ones((Nhidden, T))\n",
    "        \n",
    "        delta[:,0] = np.log2((Pi.T * b[:,0]))\n",
    "        \n",
    "        alpha, beta, gamma, logProb, Z = ForwardBackwardSmoothing(A, b, Pi, Nhidden, T)\n",
    "        marginals = SmoothedMarginals(A, b, alpha, beta, T, Nhidden)\n",
    "        \n",
    "        for t in range(1, T):\n",
    "            for j in range(Nhidden):\n",
    "                delta[j, t], psi[j, t] = max(delta[:, t-1] + np.log2(A[:,j])), np.argmax(delta[:, t-1] + np.log2(A[:,j])) \n",
    "                delta[j, t] = delta[j, t] + np.log2(b[j,t])\n",
    "                \n",
    "        #traceback\n",
    "        S[n][T-1] = np.argmax(delta[:,T-1])\n",
    "        for t in range(T-2, -1, -1):\n",
    "            S[n][t] = psi[int(S[n][t+1])][t+1]\n",
    "                                                                        \n",
    "    return S+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convergence (Gaussian):  True\n",
      "Convergence (multinomial):  False\n",
      "Convergence (Gaussian):  True\n",
      "Convergence (multinomial):  False\n",
      "*** Viterbi decoding accuracy (Gaussian): 0.9995\n",
      "*** Viterbi decoding accuracy (Multinomial): 0.5975\n"
     ]
    }
   ],
   "source": [
    "with np.load('init_gaussian.npz') as f:\n",
    "    init_g = InitGaussian(f['arr_0'], f['arr_1'], f['arr_2'], f['arr_3'])\n",
    "    \n",
    "with np.load('init_multinomial.npz') as f:\n",
    "    init_m = InitMultinomial(f['arr_0'], f['arr_1'], f['arr_2'])\n",
    "    \n",
    "with np.load('data_gaussian.npz') as f:\n",
    "    Y_c, S_c = f['arr_0'], f['arr_1']\n",
    "\n",
    "with np.load('data_multinomial.npz') as f:\n",
    "    Y_d, S_d = f['arr_0'], f['arr_1']\n",
    "    \n",
    "A_g, Means_g, Variances_g, Pi_g = EM_estimate_gaussian(Y_c, 2, 100, 1e-6, init_g)\n",
    "A_m, B_m, Pi_m = EM_estimate_multinomial(Y_d, 2, 100, 1e-6, init_m)\n",
    "\n",
    "S_g = ViterbiDecode(Y_c, 2, 'gauss', init_g)\n",
    "S_m = ViterbiDecode(Y_d, 2, 'multinomial', init_m)\n",
    "\n",
    "print('*** Viterbi decoding accuracy (Gaussian): {}'.format( (S_c == S_g).sum() / S_c.size ))\n",
    "print('*** Viterbi decoding accuracy (Multinomial): {}'.format( (S_d == S_m).sum() / S_d.size ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A = [[ 0.39067931  0.60932069]\n",
      " [ 0.38883957  0.61116043]]\n",
      "Means= [[ 0.09215062]\n",
      " [ 5.00272057]]\n",
      "Variances= [[ 0.40331952]\n",
      " [ 0.82207155]]\n",
      "pi= [[ 0.56011078]\n",
      " [ 0.43988922]]\n"
     ]
    }
   ],
   "source": [
    "print('A =', A_g)\n",
    "print('Means=', Means_g)\n",
    "print('Variances=', Variances_g)\n",
    "print('pi=', Pi_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A = [[ 0.38662667  0.61337333]\n",
      " [ 0.38662667  0.61337333]]\n",
      "B= [[ 0.11980829  0.26147544  0.04246297  0.03442291  0.10120943  0.44062095]\n",
      " [ 0.13510802  0.04664145  0.18213438  0.17774353  0.14615633  0.3122163 ]]\n",
      "pi= [[ 0.33914967]\n",
      " [ 0.66085033]]\n"
     ]
    }
   ],
   "source": [
    "print('A =', A_m)\n",
    "print('B=', B_m)\n",
    "print('pi=', Pi_m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As convergence is not reached within 100 iterations for the multinomial case, the lower performance compared to the Gaussian model is expected."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
